{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive notebook of `helmpy` examples\n",
    "\n",
    "The mathematics of the model which `helmpy` uses are described in detail here: [https://doi.org/10.1101/2019.12.17.19013490]\n",
    "\n",
    "In this notebook we will run through all of the features in `helmpy` as well as providing insights into its construction along the way. Throughout we shall be running the algorithm only once to generate our plots, however for best results it is recommended to rerun `helmpy` a few more times (fewer than $10$ iterations) and to combine the output. Because a number of realisations may be run at once (e.g., $250$), this can lead to very good statistics in the output (overall $10 \\times 250$ realisations) with high efficiency.\n",
    "\n",
    "First we must tell the system where the `helmpy` directory is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_to_helmpy = '/Users/Rob/work/helmpy' # Give your path to helmpy here\n",
    "sys.path.append(path_to_helmpy + '/source/') \n",
    "from helmpy import helmpy\n",
    "\n",
    "# These modules are not necessary to run helmpy alone but will be useful for our demonstrations\n",
    "\n",
    "# LEAVE THESE IMPORTS COMMENTED AS THEY ARE FOR PRODUCING LaTeX-STYLE FIGURES ONLY\n",
    "#import matplotlib as mpl\n",
    "#mpl.use('Agg')\n",
    "#mpl.rc('font',family='CMU Serif')\n",
    "#mpl.rcParams['xtick.labelsize'] = 15\n",
    "#mpl.rcParams['ytick.labelsize'] = 15\n",
    "#mpl.rcParams['axes.labelsize'] = 20\n",
    "#from matplotlib import rc\n",
    "#rc('text',usetex=True)\n",
    "#rc('text.latex',preamble=r'\\usepackage{mathrsfs}')\n",
    "#rc('text.latex',preamble=r'\\usepackage{sansmath}')\n",
    "# LEAVE THESE IMPORTS COMMENTED AS THEY ARE FOR PRODUCING LaTeX-STYLE FIGURES ONLY\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as spec\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Soil-transmitted helminths (STH)\n",
    "\n",
    "## 1.1 Running basic simulations in three steps\n",
    "\n",
    "### Step 1\n",
    "\n",
    "Initialise the `helmpy` class by specifying a disease type and working directory..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp = helmpy(\n",
    "            'STH',                              # Set the disease type - types available are: 'STH', 'SCH' and 'LF'\n",
    "            path_to_helmpy,                     # Set the path to the working directory\n",
    "            suppress_terminal_output=False      # Set this to 'True' to remove terminal messages\n",
    "            )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Set the parameters and initial conditions for the simulation. List sizes are variable depending on the number of groupings one wishes to apply. Hence age, gender and all other stratifications within a cluster may be programmed in straightforwardly and each grouping may be identified within a given cluster by a spatial index. The available basic options to vary are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.parameter_dictionary['mu'] = [0.014,0.014]   # Human death rate (per year)\n",
    "hp.parameter_dictionary['mu1'] = [0.5,0.5]      # Adult worm death rate (per year)\n",
    "hp.parameter_dictionary['mu2'] = [26.0,26.0]    # Reservoir (eggs and larvae) death rate (per year)\n",
    "hp.parameter_dictionary['R0'] = [3.5,2.1]       # Basic reproduction number within grouping\n",
    "#hp.parameter_dictionary['R0'] = [3.5,3.0]       # Basic reproduction number within grouping\n",
    "hp.parameter_dictionary['k'] = [0.3,0.5]        # Inverse-clumping factor within grouping\n",
    "hp.parameter_dictionary['gam'] = [0.08,0.08]    # Density dependent fecundity: z = exp(-gam)\n",
    "hp.parameter_dictionary['Np'] = [300,350]       # Number of people within grouping   \n",
    "hp.parameter_dictionary['spi'] = [1,2]          # Spatial index number of grouping\n",
    "\n",
    "hp.initial_conditions['M'] = [2.9,2.1]          # Initial mean total worm burden within grouping\n",
    "#hp.initial_conditions['M'] = [2.9,3.1]          # Initial mean total worm burden within grouping\n",
    "hp.initial_conditions['FOI'] = [1.25,1.1]       # Initial force of infection (per year) within grouping\n",
    "#hp.initial_conditions['FOI'] = [1.25,1.5]       # Initial force of infection (per year) within grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are running two non-interacting clusters each with a single grouping of people which both contain different numbers of people, different initial conditions, different parasite clumping (controlled through $k$) and different overall values of parasite uptake: here combined into a parameter we call `R0` - note that these are different to the true $R_0$ value in the cluster which can be computed from the weighted sum of these sub-`R0` values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "Run the simulation by setting the key numerical parameters. For comparison we will also run the mean field and mean field stochastic codes (which are much faster) alongside the full simulation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 100.0                               # Set the total time of the run in years\n",
    "realisations = 250                            # Set the number of stochastic realisations for the model\n",
    "do_nothing_timescale = 0.01                   # Set a timescale (in years) short enough such that an individual \n",
    "                                              # is expected to stay in the same state\n",
    "    \n",
    "output_filename = 'default_example'           # Set a filename for the data to be output in data/\n",
    "            \n",
    "output_timesteps = [1000,2000,3000,9000]      # Optional - output binned worm burdens over whole population \n",
    "                                              # and realisations after a specified number of steps in time  \n",
    "hp.run_full_stoch(         \n",
    "                  runtime,          \n",
    "                  realisations,  \n",
    "                  do_nothing_timescale,\n",
    "                  output_filename,  \n",
    "                  timesteps_snapshot=output_timesteps\n",
    "                  )\n",
    "\n",
    "hp.run_meanfield_stoch(         \n",
    "                       runtime, \n",
    "                       realisations,\n",
    "                       do_nothing_timescale,                \n",
    "                       'meanfield_stoch_' + output_filename,\n",
    "                       timesteps_snapshot=output_timesteps\n",
    "                       )\n",
    "\n",
    "hp.run_meanfield(         \n",
    "                 runtime,  \n",
    "                 do_nothing_timescale,                \n",
    "                 'meanfield_' + output_filename       \n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run all three models, let us now read in the data and compare the ensemble mean and variance of the mean worm burden realisations in each cluster.\n",
    "\n",
    "First we compare the predictions of the mean field model to those of the full simulation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "example_output_data = np.loadtxt(path_to_helmpy + '/data/' + output_filename + '.txt')\n",
    "example_meanfield_output_data = np.loadtxt(path_to_helmpy + '/data/' + 'meanfield_' + output_filename + '.txt')\n",
    "\n",
    "# Mean of ensemble in cluster 1\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[1],color='r',\\\n",
    "         label=r'$R_0 =' + str(hp.parameter_dictionary['R0'][0]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hp.parameter_dictionary['k'][0]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hp.parameter_dictionary['Np'][0]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hp.initial_conditions['M'][0]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hp.initial_conditions['FOI'][0]) + r'$')\n",
    "\n",
    "# Mean of ensemble in cluster 2\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[2],color='b',\\\n",
    "         label=r'$R_0 =' + str(hp.parameter_dictionary['R0'][1]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hp.parameter_dictionary['k'][1]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hp.parameter_dictionary['Np'][1]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hp.initial_conditions['M'][1]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hp.initial_conditions['FOI'][1]) + r'$')\n",
    "\n",
    "# Mean field of ensemble in cluster 1\n",
    "plt.plot(example_meanfield_output_data.T[0],example_meanfield_output_data.T[1],color='r',alpha=0.4) \n",
    "\n",
    "# Mean field of ensemble in cluster 2\n",
    "plt.plot(example_meanfield_output_data.T[0],example_meanfield_output_data.T[2],color='b',alpha=0.4) \n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([2.5,7.5])\n",
    "axes.set_ylabel(r'$M(t)$')\n",
    "axes.set_xlabel(r'$t-t_0$')\n",
    "\n",
    "plt.legend(fontsize = 12)\n",
    "plt.savefig(path_to_helmpy + '/plots/Mean_meanfield_comp.png',format='png',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_data = np.loadtxt(path_to_helmpy + '/data/' + output_filename + '.txt')\n",
    "example_meanfield_output_data = np.loadtxt(path_to_helmpy + '/data/' + 'meanfield_' + output_filename + '.txt')\n",
    "\n",
    "# Variance of ensemble in cluster 1\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[3],color='r',\\\n",
    "         label=r'$R_0 =' + str(hp.parameter_dictionary['R0'][0]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hp.parameter_dictionary['k'][0]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hp.parameter_dictionary['Np'][0]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hp.initial_conditions['M'][0]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hp.initial_conditions['FOI'][0]) + r'$')\n",
    "\n",
    "# Variance of ensemble in cluster 2\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[4],color='b',\\\n",
    "         label=r'$R_0 =' + str(hp.parameter_dictionary['R0'][1]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hp.parameter_dictionary['k'][1]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hp.parameter_dictionary['Np'][1]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hp.initial_conditions['M'][1]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hp.initial_conditions['FOI'][1]) + r'$')\n",
    "\n",
    "\n",
    "# Variance from mean field of ensemble in cluster 1\n",
    "plt.plot(example_meanfield_output_data.T[0],example_meanfield_output_data.T[3],color='r',alpha=0.4)\n",
    "\n",
    "# Variance from mean field of ensemble in cluster 2\n",
    "plt.plot(example_meanfield_output_data.T[0],example_meanfield_output_data.T[4],color='b',alpha=0.4)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,2.5])\n",
    "axes.set_ylabel(r'$V(t)$')\n",
    "axes.set_xlabel(r'$t-t_0$')\n",
    "\n",
    "plt.legend(fontsize = 12)\n",
    "plt.savefig(path_to_helmpy + '/plots/Var_meanfield_comp.png',format='png',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we compare the predictions of a fast stochastic model built from the mean field code to those of the full simulation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_data = np.loadtxt(path_to_helmpy + '/data/' + output_filename + '.txt')\n",
    "example_meanfield_stoch_data = np.loadtxt(path_to_helmpy + '/data/' + 'meanfield_stoch_' + output_filename + '.txt')\n",
    "\n",
    "# Mean of ensemble in cluster 1\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[1],color='r',\\\n",
    "         label=r'$R_0 =' + str(hp.parameter_dictionary['R0'][0]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hp.parameter_dictionary['k'][0]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hp.parameter_dictionary['Np'][0]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hp.initial_conditions['M'][0]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hp.initial_conditions['FOI'][0]) + r'$')\n",
    "\n",
    "# Mean of ensemble in cluster 2\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[2],color='b',\\\n",
    "         label=r'$R_0 =' + str(hp.parameter_dictionary['R0'][1]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hp.parameter_dictionary['k'][1]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hp.parameter_dictionary['Np'][1]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hp.initial_conditions['M'][1]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hp.initial_conditions['FOI'][1]) + r'$')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 1\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[5],color='r')\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[7],color='r')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 2\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[6],color='b')\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[8],color='b')\n",
    "\n",
    "# Mean of mean-field stochastic ensemble in cluster 1\n",
    "plt.plot(example_meanfield_stoch_data.T[0],example_meanfield_stoch_data.T[1],color='r',alpha=0.4)\n",
    "\n",
    "# Mean of mean-field stochastic ensemble in cluster 2\n",
    "plt.plot(example_meanfield_stoch_data.T[0],example_meanfield_stoch_data.T[2],color='b',alpha=0.4)\n",
    "\n",
    "# 68% CLs of mean-field stochastic ensemble in cluster 1\n",
    "plt.plot(example_meanfield_stoch_data.T[0],example_meanfield_stoch_data.T[5],color='r',alpha=0.4)\n",
    "plt.plot(example_meanfield_stoch_data.T[0],example_meanfield_stoch_data.T[7],color='r',alpha=0.4)\n",
    "\n",
    "# 68% CLs of mean-field stochastic ensemble in cluster 2\n",
    "plt.plot(example_meanfield_stoch_data.T[0],example_meanfield_stoch_data.T[6],color='b',alpha=0.4)\n",
    "plt.plot(example_meanfield_stoch_data.T[0],example_meanfield_stoch_data.T[8],color='b',alpha=0.4)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,9.5])\n",
    "axes.set_ylabel(r'$m(t)$')\n",
    "axes.set_xlabel(r'$t-t_0$')\n",
    "\n",
    "plt.legend(fontsize = 12, loc = 9)\n",
    "plt.savefig(path_to_helmpy + '/plots/meanfield_stoch_comp.png',format='png',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us also have a look at some snapshots in time of the mean worm burden distribution from the full simulation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_values = [1000,2000,3000,9000]\n",
    "alphlist = [0.2,0.4,0.6,1.0]\n",
    "\n",
    "for ti in range(0,len(time_values)):\n",
    "    wb_data = np.loadtxt(path_to_helmpy + '/data/' + output_filename + \\\n",
    "                         '_snapshot_timestep_' + str(time_values[ti]) + '_cluster_1.txt')\n",
    "    bo = np.histogram((np.sum(wb_data,axis=1)/len(wb_data.T)).astype(float),bins='fd',density=True)\n",
    "    box = 0.5*(bo[1][:len(bo[0])]+bo[1][1:len(bo[0])+1])\n",
    "    boy = bo[0]\n",
    "    plt.plot(box[boy!=0.0],(boy/max(boy))[boy!=0.0],color='r',alpha=alphlist[ti])\n",
    "    \n",
    "for ti in range(0,len(time_values)):\n",
    "    wb_data = np.loadtxt(path_to_helmpy + '/data/' + output_filename + \\\n",
    "                         '_snapshot_timestep_' + str(time_values[ti]) + '_cluster_2.txt')\n",
    "    bo = np.histogram((np.sum(wb_data,axis=1)/len(wb_data.T)).astype(float),bins='fd',density=True)\n",
    "    box = 0.5*(bo[1][:len(bo[0])]+bo[1][1:len(bo[0])+1])\n",
    "    boy = bo[0]\n",
    "    plt.plot(box[boy!=0.0],(boy/max(boy))[boy!=0.0],color='b',alpha=alphlist[ti])\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0.0,8.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and the equivalent from the mean field stochastic code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_values = [1000,2000,3000,9000]\n",
    "alphlist = [0.2,0.4,0.6,1.0]\n",
    "\n",
    "for ti in range(0,len(time_values)):\n",
    "    mwb_data = np.loadtxt(path_to_helmpy + '/data/'  + 'meanfield_stoch_' + output_filename + \\\n",
    "                          '_snapshot_timestep_' + str(time_values[ti]) + '_cluster_1.txt')\n",
    "    bo = np.histogram(mwb_data.astype(float),bins='fd',density=True)\n",
    "    box = 0.5*(bo[1][:len(bo[0])]+bo[1][1:len(bo[0])+1])\n",
    "    boy = bo[0]\n",
    "    plt.plot(box[boy!=0.0],(boy/max(boy))[boy!=0.0],color='r',alpha=alphlist[ti])\n",
    "    \n",
    "for ti in range(0,len(time_values)):\n",
    "    mwb_data = np.loadtxt(path_to_helmpy + '/data/'  + 'meanfield_stoch_' + output_filename + \\\n",
    "                          '_snapshot_timestep_' + str(time_values[ti]) + '_cluster_2.txt')\n",
    "    bo = np.histogram(mwb_data.astype(float),bins='fd',density=True)\n",
    "    box = 0.5*(bo[1][:len(bo[0])]+bo[1][1:len(bo[0])+1])\n",
    "    boy = bo[0]\n",
    "    plt.plot(box[boy!=0.0],(boy/max(boy))[boy!=0.0],color='b',alpha=alphlist[ti])\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0.0,8.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Adding inter-cluster migration\n",
    "\n",
    "To begin with, let us create a new instance of `helmpy` with the same parameters and initial conditions set for comparison in order to continue working distinctly from the previous section..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpmig = helmpy(\n",
    "              'STH',                              # Set the disease type - types available are: 'STH', 'SCH' and 'LF'\n",
    "              path_to_helmpy,                     # Set the path to the working directory\n",
    "              suppress_terminal_output=False      # Set this to 'True' to remove terminal messages\n",
    "              ) \n",
    "\n",
    "hpmig.parameter_dictionary['mu'] = [0.014,0.014]   # Human death rate (per year)\n",
    "hpmig.parameter_dictionary['mu1'] = [0.5,0.5]      # Adult worm death rate (per year)\n",
    "hpmig.parameter_dictionary['mu2'] = [26.0,26.0]    # Reservoir (eggs and larvae) death rate (per year)\n",
    "hpmig.parameter_dictionary['R0'] = [3.5,2.1]       # Basic reproduction number within grouping\n",
    "hpmig.parameter_dictionary['k'] = [0.3,0.5]        # Inverse-clumping factor within grouping\n",
    "hpmig.parameter_dictionary['gam'] = [0.08,0.08]    # Density dependent fecundity: z = exp(-gam)\n",
    "hpmig.parameter_dictionary['Np'] = [300,350]       # Number of people within grouping   \n",
    "hpmig.parameter_dictionary['spi'] = [1,2]          # Spatial index number of grouping\n",
    "\n",
    "hpmig.initial_conditions['M'] = [2.9,2.1]          # Initial mean total worm burden within grouping\n",
    "hpmig.initial_conditions['FOI'] = [1.25,1.1]       # Initial force of infection (per year) within grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run the basic simulations above for two separate clusters, let us now introduce an inter-cluster migration effect by specifying non-zero migratory rate matrices with the following definitions: Components $(r_+)_{ij}$ and $(r_-)_{ij}$ are the migration rates (per year) of individuals in to cluster $i$ from cluster $j$ and out of cluster $i$ from cluster $j$, respectively. Hence, the matrices for two clusters are\n",
    "\n",
    "$${\\bf r}_+ = \\begin{bmatrix} (r_+)_{11} & (r_+)_{12} \\\\ (r_+)_{21} & (r_+)_{22}\\end{bmatrix}\\,,\\qquad {\\bf r}_- = \\begin{bmatrix} (r_-)_{11} & (r_-)_{12} \\\\ (r_-)_{21} & (r_-)_{22}\\end{bmatrix}\\,,$$\n",
    "\n",
    "and one may deduce that $(r_+)_{ij}=(r_-)_{ji}$. Let us now set these matrices in the parameters of our `helmpy` instance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpmig.parameter_dictionary['r+'] = [[0.0,0.0],[52.0,0.0]]  # Migration matrix - the migration rate in (per year)\n",
    "hpmig.parameter_dictionary['r-'] = [[0.0,52.0],[0.0,0.0]]  # Migration matrix - the migration rate out (per year)\n",
    "hpmig.parameter_dictionary['Nm'] = [1]                     # Number of migrants per event (global parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the migrations have been set, we may simply rerun the code as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 100.0                               # Set the total time of the run in years\n",
    "realisations = 250                            # Set the number of stochastic realisations for the model\n",
    "do_nothing_timescale = 0.01                   # Set a timescale (in years) short enough such that an individual \n",
    "                                              # is expected to stay in the same state\n",
    "    \n",
    "output_filename = 'default_example'           # Set a filename for the data to be output in data/\n",
    "            \n",
    "output_timesteps = [1000,2000,3000,9000]      # Optional - output binned worm burdens over whole population \n",
    "                                              # and realisations after a specified number of steps in time  \n",
    "\n",
    "hpmig.run_full_stoch(         \n",
    "                    runtime,          \n",
    "                    realisations,  \n",
    "                    do_nothing_timescale,\n",
    "                    'mig_' + output_filename,  \n",
    "                    timesteps_snapshot=output_timesteps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_data = np.loadtxt(path_to_helmpy + '/data/' + output_filename + '.txt')\n",
    "example_output_data_mig = np.loadtxt(path_to_helmpy + '/data/' + 'mig_' + output_filename + '.txt')\n",
    "\n",
    "# Mean of ensemble in cluster 1 with migration\n",
    "plt.plot(example_output_data_mig.T[0],example_output_data_mig.T[1],color='r',\\\n",
    "         label=r'$R_0 =' + str(hpmig.parameter_dictionary['R0'][0]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hpmig.parameter_dictionary['k'][0]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hpmig.parameter_dictionary['Np'][0]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hpmig.initial_conditions['M'][0]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hpmig.initial_conditions['FOI'][0]) + r'$')\n",
    "\n",
    "# Mean of ensemble in cluster 2 with migration\n",
    "plt.plot(example_output_data_mig.T[0],example_output_data_mig.T[2],color='b',\\\n",
    "         label=r'$R_0 =' + str(hpmig.parameter_dictionary['R0'][1]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hpmig.parameter_dictionary['k'][1]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hpmig.parameter_dictionary['Np'][1]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hpmig.initial_conditions['M'][1]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hpmig.initial_conditions['FOI'][1]) + r'$')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 1 with migration\n",
    "plt.plot(example_output_data_mig.T[0],example_output_data_mig.T[5],color='r')\n",
    "plt.plot(example_output_data_mig.T[0],example_output_data_mig.T[7],color='r')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 2 with migration\n",
    "plt.plot(example_output_data_mig.T[0],example_output_data_mig.T[6],color='b')\n",
    "plt.plot(example_output_data_mig.T[0],example_output_data_mig.T[8],color='b')\n",
    "\n",
    "# Mean of ensemble in cluster 1 \n",
    "plt.plot(example_output_data.T[0],example_output_data.T[1],color='r',alpha=0.4)\n",
    "\n",
    "# Mean of ensemble in cluster 2\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[2],color='b',alpha=0.4)\n",
    "\n",
    "# 68% CLs of ensemble in cluster 1\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[5],color='r',alpha=0.4)\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[7],color='r',alpha=0.4)\n",
    "\n",
    "# 68% CLs of ensemble in cluster 2\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[6],color='b',alpha=0.4)\n",
    "plt.plot(example_output_data.T[0],example_output_data.T[8],color='b',alpha=0.4)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,8.5])\n",
    "axes.set_ylabel(r'$m(t)$')\n",
    "axes.set_xlabel(r'$t-t_0$')\n",
    "\n",
    "plt.legend(fontsize = 12, loc = 9)\n",
    "plt.text(50.0,0.5,r'$(r_+,r_-)=(' + \\\n",
    "         str(np.round(hpmig.parameter_dictionary['r+'][0][1]/ \\\n",
    "                      hpmig.parameter_dictionary['mu2'][0],decimals=1)) + '\\mu_2,' + \\\n",
    "         str(np.round(hpmig.parameter_dictionary['r-'][0][1]/ \\\n",
    "                      hpmig.parameter_dictionary['mu2'][0],decimals=1)) + '\\mu_2)$', \\\n",
    "          color='r',fontsize=15)\n",
    "plt.savefig(path_to_helmpy + '/plots/mig_comp.png',format='png',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let once again have a look at some snapshots in time of the mean worm burden distribution from the full simulation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_values = [1000,2000,3000,9000]\n",
    "alphlist = [0.2,0.4,0.6,1.0]\n",
    "\n",
    "for ti in range(0,len(time_values)):\n",
    "    wb_data = np.loadtxt(path_to_helmpy + '/data/' + 'mig_' + output_filename + \\\n",
    "                         '_snapshot_timestep_' + str(time_values[ti]) + '_cluster_1.txt')\n",
    "    bo = np.histogram((np.sum(wb_data,axis=1)/len(wb_data.T)).astype(float),bins='fd',density=True)\n",
    "    box = 0.5*(bo[1][:len(bo[0])]+bo[1][1:len(bo[0])+1])\n",
    "    boy = bo[0]\n",
    "    plt.plot(box[boy!=0.0],(boy/max(boy))[boy!=0.0],color='r',alpha=alphlist[ti])\n",
    "    \n",
    "for ti in range(0,len(time_values)):\n",
    "    wb_data = np.loadtxt(path_to_helmpy + '/data/' + 'mig_' + output_filename + \\\n",
    "                         '_snapshot_timestep_' + str(time_values[ti]) + '_cluster_2.txt')\n",
    "    bo = np.histogram((np.sum(wb_data,axis=1)/len(wb_data.T)).astype(float),bins='fd',density=True)\n",
    "    box = 0.5*(bo[1][:len(bo[0])]+bo[1][1:len(bo[0])+1])\n",
    "    boy = bo[0]\n",
    "    plt.plot(box[boy!=0.0],(boy/max(boy))[boy!=0.0],color='b',alpha=alphlist[ti])\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([0.0,8.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Adding treatment rounds\n",
    "\n",
    "The `helmpy` class models rounds of mass drug administration (MDA) through Bernoulli trials with a given probability of reducing an individual's worm burden to 0. This is effectively a 'random compliance' model. Let us use this feature to examine the influence of migration on the efficacy of treatment using the system setup from before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the Boolean to see the effect of migration for simplicity...\n",
    "with_migration = False\n",
    "\n",
    "hptrt = helmpy(\n",
    "              'STH',                              # Set the disease type - types available are: 'STH', 'SCH' and 'LF'\n",
    "              path_to_helmpy,                     # Set the path to the working directory\n",
    "              suppress_terminal_output=False      # Set this to 'True' to remove terminal messages\n",
    "              ) \n",
    "\n",
    "hptrt.parameter_dictionary['mu'] = [0.014,0.014]   # Human death rate (per year)\n",
    "hptrt.parameter_dictionary['mu1'] = [0.5,0.5]      # Adult worm death rate (per year)\n",
    "hptrt.parameter_dictionary['mu2'] = [26.0,26.0]    # Reservoir (eggs and larvae) death rate (per year)\n",
    "hptrt.parameter_dictionary['R0'] = [3.5,2.1]       # Basic reproduction number within grouping\n",
    "hptrt.parameter_dictionary['k'] = [0.3,0.5]        # Inverse-clumping factor within grouping\n",
    "hptrt.parameter_dictionary['gam'] = [0.08,0.08]    # Density dependent fecundity: z = exp(-gam)\n",
    "hptrt.parameter_dictionary['Np'] = [300,350]       # Number of people within grouping   \n",
    "hptrt.parameter_dictionary['spi'] = [1,2]          # Spatial index number of grouping\n",
    "\n",
    "hptrt.initial_conditions['M'] = [2.9,2.1]          # Initial mean total worm burden within grouping\n",
    "hptrt.initial_conditions['FOI'] = [1.25,1.1]       # Initial force of infection (per year) within grouping\n",
    "\n",
    "if with_migration == True:\n",
    "    hptrt.parameter_dictionary['r+'] = [[0.0,0.0],[52.0,0.0]] # Migration matrix - the migration rate in (per year)\n",
    "    hptrt.parameter_dictionary['r-'] = [[0.0,52.0],[0.0,0.0]] # Migration matrix - the migration rate out (per year)\n",
    "    hptrt.parameter_dictionary['Nm'] = [10]                   # Number of migrants per event (global parameter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having initialised a new instance, let us define 3 rounds of MDA with 60% coverage at years 15, 16 and 17 to see the effect this has on the disease..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_coverages = [[0.6,0.6,0.6],         # A list of lists matching the chosen groupings which gives \n",
    "                       [0.6,0.6,0.6]]         # the effective coverage fraction in each\n",
    "\n",
    "treatment_times = [15.0,16.0,17.0]            # A list of treatment times for all clusters\n",
    "\n",
    "hptrt.add_treatment_prog(\n",
    "                        treatment_times,         \n",
    "                        treatment_coverages=treatment_coverages,\n",
    "                        drug_efficacy=1.0\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the simulation which will output both the mean worm burden dynamics (as before) and the final prevalence realisations at the last round of treatment and at the absolute end of the runs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 100.0                               # Set the total time of the run in years\n",
    "realisations = 250                            # Set the number of stochastic realisations for the model\n",
    "do_nothing_timescale = 0.01                   # Set a timescale (in years) short enough such that an individual \n",
    "                                              # is expected to stay in the same state\n",
    "    \n",
    "output_filename = 'default_example'           # Set a filename for the data to be output in data/\n",
    "            \n",
    "output_timesteps = [1000,2000,3000,9000]      # Optional - output binned worm burdens over whole population \n",
    "                                              # and realisations after a specified number of steps in time\n",
    "\n",
    "if with_migration == False:   \n",
    "    hptrt.run_full_stoch(         \n",
    "                        runtime,          \n",
    "                        realisations,  \n",
    "                        do_nothing_timescale,\n",
    "                        'trt_' + output_filename,  \n",
    "                        timesteps_snapshot=output_timesteps\n",
    "                        )\n",
    "    \n",
    "if with_migration == True:   \n",
    "    hptrt.run_full_stoch(         \n",
    "                        runtime,          \n",
    "                        realisations,  \n",
    "                        do_nothing_timescale,\n",
    "                        'mig_trt_' + output_filename,  \n",
    "                        timesteps_snapshot=output_timesteps\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_data_mig_trt = np.loadtxt(path_to_helmpy + '/data/' + 'mig_trt_' + output_filename + '.txt')\n",
    "example_output_data_trt = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + '.txt')\n",
    "\n",
    "# Mean of ensemble in cluster 1 with migration\n",
    "plt.plot(example_output_data_mig_trt.T[0],example_output_data_mig_trt.T[1],color='r',\\\n",
    "         label=r'$R_0 =' + str(hptrt.parameter_dictionary['R0'][0]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hptrt.parameter_dictionary['k'][0]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hptrt.parameter_dictionary['Np'][0]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hptrt.initial_conditions['M'][0]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hptrt.initial_conditions['FOI'][0]) + r'$')\n",
    "\n",
    "# Mean of ensemble in cluster 2 with migration\n",
    "plt.plot(example_output_data_mig_trt.T[0],example_output_data_mig_trt.T[2],color='b',\\\n",
    "         label=r'$R_0 =' + str(hptrt.parameter_dictionary['R0'][1]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hptrt.parameter_dictionary['k'][1]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hptrt.parameter_dictionary['Np'][1]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hptrt.initial_conditions['M'][1]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hptrt.initial_conditions['FOI'][1]) + r'$')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 1 with migration\n",
    "plt.plot(example_output_data_mig_trt.T[0],example_output_data_mig_trt.T[5],color='r')\n",
    "plt.plot(example_output_data_mig_trt.T[0],example_output_data_mig_trt.T[7],color='r')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 2 with migration\n",
    "plt.plot(example_output_data_mig_trt.T[0],example_output_data_mig_trt.T[6],color='b')\n",
    "plt.plot(example_output_data_mig_trt.T[0],example_output_data_mig_trt.T[8],color='b')\n",
    "\n",
    "# Mean of ensemble in cluster 1 \n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[1],color='r',alpha=0.4)\n",
    "\n",
    "# Mean of ensemble in cluster 2\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[2],color='b',alpha=0.4)\n",
    "\n",
    "# 68% CLs of ensemble in cluster 1\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[5],color='r',alpha=0.4)\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[7],color='r',alpha=0.4)\n",
    "\n",
    "# 68% CLs of ensemble in cluster 2\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[6],color='b',alpha=0.4)\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[8],color='b',alpha=0.4)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,8.5])\n",
    "axes.set_ylabel(r'$m(t)$')\n",
    "axes.set_xlabel(r'$t-t_0$')\n",
    "\n",
    "plt.legend(fontsize = 12, loc = 9)\n",
    "#plt.text(50.0,1.0,r'$(r_+,r_-)=(' + str(hptrt.parameter_dictionary['r+'][0][1]) + ',' + \\\n",
    "#                                    str(hptrt.parameter_dictionary['r-'][0][1]) + ')$',color='r',fontsize=15)\n",
    "plt.savefig(path_to_helmpy + '/plots/treat_comp.png',format='png',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_lasttreat_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_1.txt')\n",
    "example_final_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_1.txt')\n",
    "\n",
    "example_lasttreat_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_2.txt')\n",
    "example_final_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_2.txt')\n",
    "\n",
    "p1,b1 = np.histogram(example_lasttreat_prev_data,bins='fd')\n",
    "p2,b2 = np.histogram(example_final_prev_data,bins='fd')\n",
    "p3,b3 = np.histogram(example_lasttreat_prev_data2,bins='fd')\n",
    "p4,b4 = np.histogram(example_final_prev_data2,bins='fd')\n",
    "\n",
    "plt.plot(0.5*(b1[:len(b1)-1]+b1[1:len(b1)]),p1/max(p1),color='r',alpha=0.4)\n",
    "plt.plot(0.5*(b2[:len(b2)-1]+b2[1:len(b2)]),p2/max(p2),color='r')\n",
    "plt.plot(0.5*(b3[:len(b3)-1]+b3[1:len(b3)]),p3/max(p3),color='b',alpha=0.4)\n",
    "plt.plot(0.5*(b4[:len(b4)-1]+b4[1:len(b3)]),p4/max(p4),color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_lasttreat_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'mig_trt_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_1.txt')\n",
    "example_final_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'mig_trt_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_1.txt')\n",
    "\n",
    "example_lasttreat_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'mig_trt_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_2.txt')\n",
    "example_final_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'mig_trt_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_2.txt')\n",
    "\n",
    "p1,b1 = np.histogram(example_lasttreat_prev_data,bins='fd')\n",
    "p2,b2 = np.histogram(example_final_prev_data,bins='fd')\n",
    "p3,b3 = np.histogram(example_lasttreat_prev_data2,bins='fd')\n",
    "p4,b4 = np.histogram(example_final_prev_data2,bins='fd')\n",
    "\n",
    "plt.plot(0.5*(b1[:len(b1)-1]+b1[1:len(b1)]),p1/max(p1),color='r',alpha=0.4)\n",
    "plt.plot(0.5*(b2[:len(b2)-1]+b2[1:len(b2)]),p2/max(p2),color='r')\n",
    "plt.plot(0.5*(b3[:len(b3)-1]+b3[1:len(b3)]),p3/max(p3),color='b',alpha=0.4)\n",
    "plt.plot(0.5*(b4[:len(b4)-1]+b4[1:len(b4)]),p4/max(p4),color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Adding individual non-compliance to the treatment rounds\n",
    "\n",
    "The `helmpy` class also contains models for systematic individual non-compliance to the MDA programme in the form of the conditional probabilities of: individuals in the $j$-th grouping being treated in the $n$-th round given treatment in the $(n-1)$-th round $\\alpha^j_{n,n-1}$ and being treated in the $n$-th round given non-treatment in the $(n-1)$-th round $\\beta^j_{n,n-1}$ in a Markov model such that the probability of an individual in the $j$-th grouping is treated in the $n$-th round may be written as\n",
    "\n",
    "$$p_{j,n} = \\alpha^j_{n,n-1} p_{j,n-1} + \\beta^j_{n,n-1} [1 - p_{j,n-1}]\\,.$$\n",
    "\n",
    "To demonstrate how this model impacts the MDA scenario we have already been working with, let us once again re-initialise `helmpy`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpcom = helmpy(\n",
    "              'STH',                              # Set the disease type - types available are: 'STH', 'SCH' and 'LF'\n",
    "              path_to_helmpy,                     # Set the path to the working directory\n",
    "              suppress_terminal_output=False      # Set this to 'True' to remove terminal messages\n",
    "              ) \n",
    "\n",
    "hpcom.parameter_dictionary['mu'] = [0.014,0.014]   # Human death rate (per year)\n",
    "hpcom.parameter_dictionary['mu1'] = [0.5,0.5]      # Adult worm death rate (per year)\n",
    "hpcom.parameter_dictionary['mu2'] = [26.0,26.0]    # Reservoir (eggs and larvae) death rate (per year)\n",
    "hpcom.parameter_dictionary['R0'] = [3.5,2.1]       # Basic reproduction number within grouping\n",
    "hpcom.parameter_dictionary['k'] = [0.3,0.5]        # Inverse-clumping factor within grouping\n",
    "hpcom.parameter_dictionary['gam'] = [0.08,0.08]    # Density dependent fecundity: z = exp(-gam)\n",
    "hpcom.parameter_dictionary['Np'] = [300,350]       # Number of people within grouping   \n",
    "hpcom.parameter_dictionary['spi'] = [1,2]          # Spatial index number of grouping\n",
    "\n",
    "hpcom.initial_conditions['M'] = [2.9,2.1]          # Initial mean total worm burden within grouping\n",
    "hpcom.initial_conditions['FOI'] = [1.25,1.1]       # Initial force of infection (per year) within grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having initialised another new instance, let us define the same 3 rounds of MDA with the same effective 60% coverage at years 15, 16 and 17 but with compliance parameters which indicate systematic individual non-compliance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_alpha_betas = [[0.6,0.4,0.97,0.05,0.97,0.05],     # A list of lists matching the chosen groupings which gives \n",
    "                    [0.6,0.4,0.97,0.05,0.97,0.05]]     # the alpha and beta parameters in the order of \n",
    "                                                       # alpha_1, beta_1, alpha_2,... in each, where the \n",
    "                                                       # first round alpha is an initial coverage fraction\n",
    "    \n",
    "treatment_times = [15.0,16.0,17.0]                     # A list of treatment times for all clusters\n",
    "\n",
    "hpcom.add_treatment_prog(\n",
    "                        treatment_times,         \n",
    "                        compliance_params=comp_alpha_betas,\n",
    "                        drug_efficacy=1.0\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to run the code and plot the same outputs at the end of the runs as before for comparison..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 100.0                               # Set the total time of the run in years\n",
    "realisations = 250                            # Set the number of stochastic realisations for the model\n",
    "do_nothing_timescale = 0.01                   # Set a timescale (in years) short enough such that an individual \n",
    "                                              # is expected to stay in the same state\n",
    "    \n",
    "output_filename = 'default_example'           # Set a filename for the data to be output in data/\n",
    "            \n",
    "output_timesteps = [1000,2000,3000,9000]      # Optional - output binned worm burdens over whole population \n",
    "                                              # and realisations after a specified number of steps in time\n",
    "\n",
    "hpcom.run_full_stoch(         \n",
    "                    runtime,          \n",
    "                    realisations,  \n",
    "                    do_nothing_timescale,\n",
    "                    'com_' + output_filename,  \n",
    "                    timesteps_snapshot=output_timesteps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output_data_com = np.loadtxt(path_to_helmpy + '/data/' + 'com_' + output_filename + '.txt')\n",
    "example_output_data_trt = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + '.txt')\n",
    "\n",
    "# Mean of ensemble in cluster 1 with non-compliance\n",
    "plt.plot(example_output_data_com.T[0],example_output_data_com.T[1],color='r',\\\n",
    "         label=r'$R_0 =' + str(hpcom.parameter_dictionary['R0'][0]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hpcom.parameter_dictionary['k'][0]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hpcom.parameter_dictionary['Np'][0]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hpcom.initial_conditions['M'][0]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hpcom.initial_conditions['FOI'][0]) + r'$')\n",
    "\n",
    "# Mean of ensemble in cluster 2 with non-compliance\n",
    "plt.plot(example_output_data_com.T[0],example_output_data_com.T[2],color='b',\\\n",
    "         label=r'$R_0 =' + str(hpcom.parameter_dictionary['R0'][1]) + r'\\,\\,' + \\\n",
    "               r'k =' + str(hpcom.parameter_dictionary['k'][1]) + r'\\,\\,' + \\\n",
    "               r'N_{\\mathrm{p}} =' + str(hpcom.parameter_dictionary['Np'][1]) + r'\\,\\,' + \\\n",
    "               r'M(t_0) =' + str(hpcom.initial_conditions['M'][1]) + r'\\,\\,' + \\\n",
    "               r'\\Lambda (t_0) =' + str(hpcom.initial_conditions['FOI'][1]) + r'$')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 1 with non-compliance\n",
    "plt.plot(example_output_data_com.T[0],example_output_data_com.T[5],color='r')\n",
    "plt.plot(example_output_data_com.T[0],example_output_data_com.T[7],color='r')\n",
    "\n",
    "# 68% CLs of ensemble in cluster 2 with non-compliance\n",
    "plt.plot(example_output_data_com.T[0],example_output_data_com.T[6],color='b')\n",
    "plt.plot(example_output_data_com.T[0],example_output_data_com.T[8],color='b')\n",
    "\n",
    "# Mean of ensemble in cluster 1 \n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[1],color='r',alpha=0.4)\n",
    "\n",
    "# Mean of ensemble in cluster 2\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[2],color='b',alpha=0.4)\n",
    "\n",
    "# 68% CLs of ensemble in cluster 1\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[5],color='r',alpha=0.4)\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[7],color='r',alpha=0.4)\n",
    "\n",
    "# 68% CLs of ensemble in cluster 2\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[6],color='b',alpha=0.4)\n",
    "plt.plot(example_output_data_trt.T[0],example_output_data_trt.T[8],color='b',alpha=0.4)\n",
    "\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,8.5])\n",
    "axes.set_ylabel(r'$m(t)$')\n",
    "axes.set_xlabel(r'$t-t_0$')\n",
    "\n",
    "plt.legend(fontsize = 12, loc = 9)\n",
    "#plt.text(50.0,1.0,r'$(r_+,r_-)=(' + str(hptrt.parameter_dictionary['r+'][0][1]) + ',' + \\\n",
    "#                                    str(hptrt.parameter_dictionary['r-'][0][1]) + ')$',color='r',fontsize=15)\n",
    "plt.savefig(path_to_helmpy + '/plots/noncomptreat_comp.png',format='png',dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_lasttreat_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_1.txt')\n",
    "example_final_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_1.txt')\n",
    "\n",
    "example_lasttreat_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_2.txt')\n",
    "example_final_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'trt_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_2.txt')\n",
    "\n",
    "p1,b1 = np.histogram(example_lasttreat_prev_data,bins='fd')\n",
    "p2,b2 = np.histogram(example_final_prev_data,bins='fd')\n",
    "p3,b3 = np.histogram(example_lasttreat_prev_data2,bins='fd')\n",
    "p4,b4 = np.histogram(example_final_prev_data2,bins='fd')\n",
    "\n",
    "plt.plot(0.5*(b1[:len(b1)-1]+b1[1:len(b1)]),p1/max(p1),color='r',alpha=0.4)\n",
    "plt.plot(0.5*(b2[:len(b2)-1]+b2[1:len(b2)]),p2/max(p2),color='r')\n",
    "plt.plot(0.5*(b3[:len(b3)-1]+b3[1:len(b3)]),p3/max(p3),color='b',alpha=0.4)\n",
    "plt.plot(0.5*(b4[:len(b4)-1]+b4[1:len(b3)]),p4/max(p4),color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_lasttreat_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'com_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_1.txt')\n",
    "example_final_prev_data = np.loadtxt(path_to_helmpy + '/data/' + 'com_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_1.txt')\n",
    "\n",
    "example_lasttreat_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'com_' + output_filename + \\\n",
    "                                                  '_lasttreat_prevalences_cluster_2.txt')\n",
    "example_final_prev_data2 = np.loadtxt(path_to_helmpy + '/data/' + 'com_' + output_filename + \\\n",
    "                                                  '_final_prevalences_cluster_2.txt')\n",
    "\n",
    "p1,b1 = np.histogram(example_lasttreat_prev_data,bins='fd')\n",
    "p2,b2 = np.histogram(example_final_prev_data,bins='fd')\n",
    "p3,b3 = np.histogram(example_lasttreat_prev_data2,bins='fd')\n",
    "p4,b4 = np.histogram(example_final_prev_data2,bins='fd')\n",
    "\n",
    "plt.plot(0.5*(b1[:len(b1)-1]+b1[1:len(b1)]),p1/max(p1),color='r',alpha=0.4)\n",
    "plt.plot(0.5*(b2[:len(b2)-1]+b2[1:len(b2)]),p2/max(p2),color='r')\n",
    "plt.plot(0.5*(b3[:len(b3)-1]+b3[1:len(b3)]),p3/max(p3),color='b',alpha=0.4)\n",
    "plt.plot(0.5*(b4[:len(b4)-1]+b4[1:len(b3)]),p4/max(p4),color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Running with ageing process between bins\n",
    "\n",
    "The default setting in `helmpy` is to not have people's worm counts 'ageing' out of bins. To include this ageing as a Poisson process which occurs at a rate controlled by the birth rate of new indviduals (with zero worms) into the cluster while keeping the number of people within each grouping constant for all time, we may adapt `helmpy` in the following way..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpage = helmpy(\n",
    "            'STH',                              # Set the disease type - types available are: 'STH', 'SCH' and 'LF'\n",
    "            path_to_helmpy,                     # Set the path to the working directory\n",
    "            suppress_terminal_output=False      # Set this to 'True' to remove terminal messages\n",
    "            )  \n",
    "\n",
    "hpage.parameter_dictionary['mu'] = [0.014,0.014]      # Human death rate (per year)\n",
    "hpage.parameter_dictionary['mu1'] = [0.5,0.5]         # Adult worm death rate (per year)\n",
    "hpage.parameter_dictionary['mu2'] = [26.0,26.0]       # Reservoir (eggs and larvae) death rate \n",
    "hpage.parameter_dictionary['R0'] = [2.1,2.1]          # Basic reproduction number within grouping\n",
    "hpage.parameter_dictionary['k'] = [0.3,0.3]           # Inverse-clumping factor within grouping\n",
    "hpage.parameter_dictionary['gam'] = [0.08,0.08]       # Density dependent fecundity: z = exp(-gam)\n",
    "hpage.parameter_dictionary['Np'] = [500,500]          # Number of people within grouping   \n",
    "hpage.parameter_dictionary['spi'] = [1,1]             # Spatial index number of grouping\n",
    "\n",
    "hpage.initial_conditions['M'] = [2.1,2.1]             # Initial mean total worm burden \n",
    "hpage.initial_conditions['FOI'] = [1.1,1.1]           # Initial force of infection (per year) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...the key modification here is to include three new parameters which are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.parameter_dictionary['ari'] = [0,1]             # Group age-ordering index - 0,1,2,3,... \n",
    "hp.parameter_dictionary['brat'] = [2.0,2.0]        # Birth rate per year into grouping 0\n",
    "hp.parameter_dictionary['Na'] = [10]               # Number of people ageing per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 100.0                               # Set the total time of the run in years\n",
    "realisations = 250                            # Set the number of stochastic realisations for the model\n",
    "do_nothing_timescale = 0.01                   # Set a timescale (in years) short enough such that an individual \n",
    "                                              # is expected to stay in the same state\n",
    "    \n",
    "output_filename = 'default_example'           # Set a filename for the data to be output in data/\n",
    "            \n",
    "output_timesteps = [1000,2000,3000,9000]      # Optional - output binned worm burdens over whole population \n",
    "                                              # and realisations after a specified number of steps in time\n",
    "\n",
    "hpcom.run_full_stoch(         \n",
    "                    runtime,          \n",
    "                    realisations,  \n",
    "                    do_nothing_timescale,\n",
    "                    'age_' + output_filename,  \n",
    "                    timesteps_snapshot=output_timesteps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including the ageing process typically increases the runtime by around 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Running large numbers of people and clusters\n",
    "\n",
    "In order to run `helmpy` with a large number of people and clusters, it is advised to reduce the number of realisations to 1 as the internal vectorisation which optimises the code cannot handle matrices that are too large. The example below tests the runtime of helmpy with 500 people each in 40 clusters including migration and treatment rounds and emphasises the simplicity of using the `helmpy` class to instanciate a run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 100.0                              \n",
    "realisations = 1                              \n",
    "do_nothing_timescale = 0.01                                                 \n",
    "    \n",
    "output_filename = 'tests'                  \n",
    "\n",
    "treatment_coverages = [[0.6,0.6,0.6] for j in range(0,40)]                          \n",
    "treatment_times = [15.0,16.0,17.0]     \n",
    "\n",
    "hptest = helmpy('STH',path_to_helmpy,suppress_terminal_output=True) \n",
    "hptest.parameter_dictionary['Np'] = [500 for j in range(0,40)]         \n",
    "hptest.parameter_dictionary['spi'] = [j for j in range(0,40)]         \n",
    "hptest.parameter_dictionary['r+'] = [[10.0*(i!=j) for i in range(0,40)] for j in range(0,40)] \n",
    "hptest.parameter_dictionary['r-'] = [[10.0*(i!=j) for i in range(0,40)] for j in range(0,40)]\n",
    "hptest.add_treatment_prog(treatment_times,treatment_coverages=treatment_coverages,drug_efficacy=1.0) \n",
    "\n",
    "start_time = time.time()\n",
    "hptest.run_full_stoch(runtime,realisations,do_nothing_timescale,output_filename)\n",
    "end_time = time.time()\n",
    "\n",
    "print('Runtime: ' + str(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...which appears to be around the 22 minute mark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Fitting the simulation to Kato-Katz data for parameter inference and forecasting \n",
    "\n",
    "Using `helmpy`, a brute force method of fitting to data with simulations can be considered. If multiple candidate realisations of the simulation are acquired and then either accepted or rejected as posterior samples over initial conditions and transmission parameters then these can either be used for parameter inference or to run in a subsequent forecasting simulation according to their likelihood.\n",
    "\n",
    "The Kato-Katz egg count data have a mean ${\\rm E}({\\rm egg})$ and variance ${\\rm Var}({\\rm egg})$, as independent summary statistics, respectively. An inference of the posterior distribution $p({\\bf s}\\vert {\\cal D})$ over the summary statistics ${\\bf s} =\\{ {\\rm E}({\\rm egg}),{\\rm Var}({\\rm egg})\\}$ given the data ${\\cal D}$ can be used to obtain the full posterior $p({\\bf x}\\vert {\\cal D})$ over the transmission parameters and initial conditions ${\\bf x}$ given the data ${\\cal D}$ in the following way\n",
    "\n",
    "$p({\\bf x}\\vert {\\cal D})=\\int p({\\bf x},{\\bf s} \\vert {\\cal D}) \\, {\\rm d}{\\bf s}$\n",
    "\n",
    "$\\qquad \\quad =\\int p({\\bf x} \\vert {\\bf s}) p({\\bf s} \\vert {\\cal D}) \\, {\\rm d}{\\bf s}$\n",
    "\n",
    "$\\qquad \\quad =\\int p({\\bf s}\\vert {\\bf x}) p({\\bf x}) \\frac{p({\\bf s} \\vert {\\cal D})}{p({\\bf s})} {\\rm d}{\\bf s}\\,.$\n",
    "\n",
    "Having marginalised over the inferred variance ${\\rm Var}({\\rm egg})$, which is assumed to be due to Kato-Katz diagnostic error, the subset of egg count means $\\tilde{{\\bf s}} =\\{ {\\rm E}({\\rm egg})\\}$ can simulated as a function of the ${\\bf x}$, which we indicate by ${\\bf f}({\\bf x})$. For modelling reasons, the functional form of $p(\\tilde{{\\bf s}}\\vert {\\bf x})$ is assumed to be Gaussian\n",
    "\n",
    "$$p({\\bf s}\\vert {\\bf x}) = \\frac{1}{\\sqrt{\\prod_i2\\pi \\epsilon^2}}\\exp \\left\\{ -\\sum_i\\frac{[{\\rm f}_i(x_i)-\\tilde{s}_i]^2}{2\\epsilon^2}\\right\\}\\,,$$\n",
    "\n",
    "with prior $p({\\bf x})$ and $\\frac{p(\\tilde{{\\bf s}} \\vert {\\cal D})}{p(\\tilde{{\\bf s}})}$ is proportional to the posterior over the summary parameters given the data. Furthermore, the simulator likelihood has a tolerance scale parameter $\\epsilon$ which is unknown, therefore we need to input a range of its possible values (which we should do below) and select the one for which there is the greatest evidence.\n",
    "\n",
    "The likelihood is automatically by `helmpy` once the summary statistics have been inferred and a subsequent simulation with parameter/initial condition samples has been run. To begin with our example, then we must first create a batch of mock Kato-Katz egg counts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanegg = 30.0\n",
    "varegg = 8000.0\n",
    "kksamps = np.random.negative_binomial(meanegg**2.0/np.abs(varegg-meanegg),meanegg/varegg,size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done this, `helmpy` must then be initialised and given the necessary information for the fit, including the Kato-Katz $\\lambda_{\\rm epg}$ parameter and a list of tolerance-to-fitting parameters $\\epsilon$ (discussed above)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpfit = helmpy('STH',path_to_helmpy,suppress_terminal_output=False)  \n",
    "\n",
    "hpfit.parameter_dictionary['Np'] = [1000]             # Number of people within grouping   \n",
    "hpfit.parameter_dictionary['spi'] = [1]               # Spatial index number of grouping\n",
    "\n",
    "hpfit.data_specific_parameters['KatoKatz'] = [3.0]                             # Kato-Katz egg count lambda_epg \n",
    "hpfit.data_specific_parameters['tolerances'] = [10.0**(-10.0+(float(i)*0.5)) \\ # Range of possible tolerances input\n",
    "                                                for i in range(0,20)]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having specified to this `helmpy` instance that it is reading Kato-Katz data through setting a value for `data_specific_parameters['KatoKatz']`, all we now need to to is run `fit_data`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_from_file = [kksamps]                 # Input the data in a list structure equivalent to the input parameters\n",
    "\n",
    "output_filename = 'default_example'        # Set a filename for the data to be output\n",
    "\n",
    "walker_initconds = [[30.0,1.0],[9.0,0.1]]  # Parameter initial conditions [centre,width] for the ensemble MC walkers\n",
    "\n",
    "plot_labels = ['Egg Mean','ln-Variance']   # Option to list a set of variable names (strings) in the same order \n",
    "\n",
    "hpfit.fit_data(data_from_file,           \n",
    "               walker_initconds,         \n",
    "               output_filename,          \n",
    "               output_corner_plot=True,  \n",
    "               plot_labels=plot_labels,  \n",
    "               num_walkers=100,          \n",
    "               num_iterations=500)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output for these inferred summary parameters has been stored in a text file but the current `helmpy` instance has also automatically stored these samples for comparison to simulation runs using the marginalised likelihood method described at the start of this subsection. To generate samples of the parameters/initial conditions and their corresponding likelihoods with respect to the data, and hence obtain posterior samples, all we need to is run this same instance of `helmpy` (using the parameter samples feature) and an output with likelihoods for each realisation will be generated. First we set the other parameters and include ageing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpfit.parameter_dictionary['mu'] = [0.014]         # Human death rate (per year)\n",
    "hpfit.parameter_dictionary['mu1'] = [0.5]          # Adult worm death rate (per year)\n",
    "hpfit.parameter_dictionary['mu2'] = [26.0]         # Reservoir (eggs and larvae) death rate\n",
    "hpfit.parameter_dictionary['ari'] = [0]            # Group age-ordering index - 0,1,2,3,... \n",
    "hpfit.parameter_dictionary['brat'] = [8.4]         # Birth rate per year into grouping 0\n",
    "hpfit.parameter_dictionary['Na'] = [10]            # Number of people ageing per event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now also make use of the `posterior_samples` feature which allows for a set of samples of parameters and initial conditions to be input..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realisations = 250                            \n",
    "\n",
    "gams = np.random.uniform(0.001,0.01,size=realisations)\n",
    "ks = 10.0**(np.random.uniform(-2.5,0.0,size=realisations))\n",
    "\n",
    "hpfit.posterior_samples['ksamps'] = [ks]                                             # Initialisation with k samples\n",
    "hpfit.posterior_samples['R0samps'] = [np.random.uniform(1.0,5.0,size=realisations)]  # Initialisation with R0 samples\n",
    "hpfit.posterior_samples['gamsamps'] = [gams]                                         # Initialisation with gam samples\n",
    "hpfit.posterior_samples['Msamps'] = [np.random.uniform(0.0,100.0,size=realisations)] # Initialisation with M samples\n",
    "hpfit.posterior_samples['FOIsamps'] = [hpfit.posterior_samples['Msamps'][0]*\\        # Initialisation with FOI samples\n",
    "                                       hpfit.parameter_dictionary['mu1'][0]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, these samples may be run and their likelihoods with respect to the data will be calculated and output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = 20.0 \n",
    "do_nothing_timescale = 0.01 \n",
    "\n",
    "hpfit.run_full_stoch(runtime,realisations,do_nothing_timescale,'fit_' + output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now read in the output and visualise the parameter samples and their likelihoods with respect to the data in some scatter plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fits = np.loadtxt(path_to_helmpy + '/data/' + 'fit_' + output_filename + '_likelihood_cluster_1.txt')\n",
    "\n",
    "maximum_index = np.argmax(spec.logsumexp(fits,axis=0))\n",
    "print('Maximum with index: ' + str(maximum_index))\n",
    "print('Corresponding log tolerance value: ' + str(np.log10(hp.data_specific_parameters['tolerances'])[maximum_index]))\n",
    "best_fits = fits[:,maximum_index]\n",
    "\n",
    "plt.plot(np.log10(hpfit.data_specific_parameters['tolerances']),spec.logsumexp(fits,axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,realisations):\n",
    "    plt.scatter(hpfit.posterior_samples['Msamps'][0][j],\\\n",
    "                np.log10(hpfit.posterior_samples['ksamps'][0][j]),\\\n",
    "                color='Red',alpha=np.exp(best_fits[j]-np.max(best_fits)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,realisations):\n",
    "    plt.scatter(hpfit.posterior_samples['R0samps'][0][j],\\\n",
    "                np.log10(hpfit.posterior_samples['ksamps'][0][j]),\\\n",
    "                color='Red',alpha=np.exp(best_fits[j]-np.max(best_fits)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,realisations):\n",
    "    plt.scatter(hpfit.posterior_samples['gamsamps'][0][j],\\\n",
    "                np.log10(hpfit.posterior_samples['ksamps'][0][j]),\\\n",
    "                color='Red',alpha=np.exp(best_fits[j]-np.max(best_fits)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
